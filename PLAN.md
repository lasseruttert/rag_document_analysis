# RAG System Enhancement Plan

## √úbersicht

Dieses Dokument definiert einen strukturierten Entwicklungsplan f√ºr die Erweiterung und Verbesserung des bestehenden RAG Document Analysis Systems. Die Implementierungen sind nach Priorit√§t und Komplexit√§t geordnet.

---

## Phase 1: Sofortige Verbesserungen (Priorit√§t: HOCH)

### P1.1: Erweiterte Dateiformate-Unterst√ºtzung ‚úÖ **ABGESCHLOSSEN**
**Ziel:** PDF und DOCX Dokumente verarbeiten k√∂nnen

**Status:** ‚úÖ **VOLLST√ÑNDIG IMPLEMENTIERT** (Januar 2025)

**Detaillierte Implementierungs-Roadmap:**

#### Schritt 1: Dependency-Setup und Testing
```bash
# Neue Dependencies installieren
pip install pdfplumber>=3.0.0 python-docx>=0.8.11 PyMuPDF>=1.23.0
```

**Update `test.py`:**
- Erweitere Package-Liste um `pdfplumber`, `docx`, `fitz` (PyMuPDF)
- F√ºge Testfunktion hinzu:
```python
def test_document_parsers():
    # Test PDF parsing mit sample PDF
    # Test DOCX parsing mit sample DOCX
    # Test error handling f√ºr korrupte Dateien
```

#### Schritt 2: PDF-Parser-Implementierung (`src/text_processor.py`)

**Neue Imports hinzuf√ºgen:**
```python
import pdfplumber
import fitz  # PyMuPDF als Fallback
from pathlib import Path
import logging
from typing import Tuple, Optional
```

**PDF-Parsing-Funktion implementieren:**
```python
def load_pdf_files(directory_path: str) -> List[Tuple[str, str, str]]:
    """
    L√§dt alle .pdf-Dateien aus einem Verzeichnis.
    Returns: List[Tuple[filename, content, file_type]]
    """
    pdf_files = []
    directory = Path(directory_path)
    
    for pdf_path in directory.glob("*.pdf"):
        try:
            # Prim√§re Methode: pdfplumber (bessere Textextraktion)
            content = extract_text_with_pdfplumber(pdf_path)
            if not content.strip():
                # Fallback: PyMuPDF f√ºr komplexe PDFs
                content = extract_text_with_pymupdf(pdf_path)
            
            if content.strip():
                pdf_files.append((pdf_path.name, content, "pdf"))
            else:
                logging.warning(f"No text extracted from {pdf_path.name}")
                
        except Exception as e:
            logging.error(f"Error processing PDF {pdf_path.name}: {e}")
            continue
    
    return pdf_files

def extract_text_with_pdfplumber(pdf_path: Path) -> str:
    """Extrahiert Text mit pdfplumber (beh√§lt Tabellen-Struktur)."""
    full_text = []
    
    with pdfplumber.open(pdf_path) as pdf:
        for page_num, page in enumerate(pdf.pages, 1):
            try:
                # Text extrahieren
                text = page.extract_text()
                if text:
                    full_text.append(f"[Seite {page_num}]\n{text}")
                
                # Tabellen separat extrahieren
                tables = page.extract_tables()
                for table_idx, table in enumerate(tables):
                    if table:
                        table_text = format_table_as_text(table)
                        full_text.append(f"[Seite {page_num}, Tabelle {table_idx + 1}]\n{table_text}")
                        
            except Exception as e:
                logging.warning(f"Error extracting page {page_num} from {pdf_path.name}: {e}")
                continue
    
    return "\n\n".join(full_text)

def format_table_as_text(table: List[List[str]]) -> str:
    """Konvertiert Tabelle zu strukturiertem Text."""
    if not table or not table[0]:
        return ""
    
    # Header-Zeile
    headers = [cell or "" for cell in table[0]]
    formatted_rows = [" | ".join(headers)]
    formatted_rows.append("-" * len(" | ".join(headers)))
    
    # Daten-Zeilen
    for row in table[1:]:
        if row:
            formatted_row = [cell or "" for cell in row]
            formatted_rows.append(" | ".join(formatted_row))
    
    return "\n".join(formatted_rows)

def extract_text_with_pymupdf(pdf_path: Path) -> str:
    """Fallback-Extraktion mit PyMuPDF."""
    full_text = []
    
    try:
        pdf_document = fitz.open(pdf_path)
        for page_num in range(pdf_document.page_count):
            page = pdf_document[page_num]
            text = page.get_text()
            if text.strip():
                full_text.append(f"[Seite {page_num + 1}]\n{text}")
        pdf_document.close()
    except Exception as e:
        logging.error(f"PyMuPDF extraction failed for {pdf_path.name}: {e}")
        return ""
    
    return "\n\n".join(full_text)
```

#### Schritt 3: DOCX-Parser-Implementierung

**DOCX-Parsing-Funktion hinzuf√ºgen:**
```python
from docx import Document
from docx.table import Table
from docx.oxml.table import CT_Tbl
from docx.oxml.text.paragraph import CT_P

def load_docx_files(directory_path: str) -> List[Tuple[str, str, str]]:
    """
    L√§dt alle .docx-Dateien aus einem Verzeichnis.
    Returns: List[Tuple[filename, content, file_type]]
    """
    docx_files = []
    directory = Path(directory_path)
    
    for docx_path in directory.glob("*.docx"):
        # Skip temporary files (~$filename.docx)
        if docx_path.name.startswith("~$"):
            continue
            
        try:
            content = extract_text_from_docx(docx_path)
            if content.strip():
                docx_files.append((docx_path.name, content, "docx"))
            else:
                logging.warning(f"No text extracted from {docx_path.name}")
                
        except Exception as e:
            logging.error(f"Error processing DOCX {docx_path.name}: {e}")
            continue
    
    return docx_files

def extract_text_from_docx(docx_path: Path) -> str:
    """Extrahiert Text und Tabellen aus DOCX-Datei."""
    try:
        document = Document(docx_path)
        full_text = []
        
        # Iteriere √ºber alle Elemente (Paragraphs und Tabellen) in Reihenfolge
        for element in document.element.body:
            if isinstance(element, CT_P):
                # Paragraph
                paragraph = next(p for p in document.paragraphs if p._element == element)
                text = paragraph.text.strip()
                if text:
                    # Formatierung beibehalten
                    if paragraph.style.name.startswith('Heading'):
                        full_text.append(f"\n## {text}\n")
                    else:
                        full_text.append(text)
                        
            elif isinstance(element, CT_Tbl):
                # Tabelle
                table = next(t for t in document.tables if t._element == element)
                table_text = extract_table_from_docx(table)
                if table_text:
                    full_text.append(f"\n[Tabelle]\n{table_text}\n")
        
        return "\n".join(full_text)
        
    except Exception as e:
        logging.error(f"Error extracting from DOCX {docx_path.name}: {e}")
        return ""

def extract_table_from_docx(table: Table) -> str:
    """Extrahiert Tabelle aus DOCX als strukturierten Text."""
    if not table.rows:
        return ""
    
    formatted_rows = []
    
    for row_idx, row in enumerate(table.rows):
        row_cells = []
        for cell in row.cells:
            cell_text = cell.text.strip().replace("\n", " ")
            row_cells.append(cell_text or "")
        
        formatted_rows.append(" | ".join(row_cells))
        
        # Header-Separator nach erster Zeile
        if row_idx == 0 and len(table.rows) > 1:
            separator = " | ".join(["-" * max(3, len(cell)) for cell in row_cells])
            formatted_rows.append(separator)
    
    return "\n".join(formatted_rows)
```

#### Schritt 4: Unified Interface Implementation

**Bestehende `load_text_files` erweitern:**
```python
def load_all_files(directory_path: str) -> List[Tuple[str, str, str]]:
    """
    L√§dt alle unterst√ºtzten Dateitypen (.txt, .pdf, .docx) aus einem Verzeichnis.
    
    Returns:
        List[Tuple[filename, content, file_type]]
    """
    all_files = []
    
    # TXT-Dateien (bestehende Logik erweitern)
    txt_files = load_text_files_extended(directory_path)
    all_files.extend(txt_files)
    
    # PDF-Dateien
    pdf_files = load_pdf_files(directory_path)
    all_files.extend(pdf_files)
    
    # DOCX-Dateien
    docx_files = load_docx_files(directory_path)
    all_files.extend(docx_files)
    
    logging.info(f"Loaded {len(all_files)} files: "
                f"{len(txt_files)} TXT, {len(pdf_files)} PDF, {len(docx_files)} DOCX")
    
    return all_files

def load_text_files_extended(directory_path: str) -> List[Tuple[str, str, str]]:
    """Erweiterte TXT-Loader mit file_type Return."""
    texts = []
    directory = Path(directory_path)
    
    for txt_path in directory.glob("*.txt"):
        try:
            with open(txt_path, 'r', encoding='utf-8') as f:
                content = f.read()
                if content.strip():
                    texts.append((txt_path.name, content, "txt"))
        except UnicodeDecodeError:
            # Fallback f√ºr andere Encodings
            try:
                with open(txt_path, 'r', encoding='latin-1') as f:
                    content = f.read()
                    if content.strip():
                        texts.append((txt_path.name, content, "txt"))
            except Exception as e:
                logging.error(f"Error reading {txt_path.name}: {e}")
    
    return texts

# Update process_documents function
def process_documents(
    directory_path: str,
    chunk_size: int = 1000,
    chunk_overlap: int = 200
) -> List[Dict[str, Any]]:
    """
    Verarbeitet alle Dokumenttypen aus einem Verzeichnis zu Chunks.
    Erweitert um file_type-Metadaten.
    """
    all_chunks = []
    loaded_files = load_all_files(directory_path)
    
    for filename, content, file_type in loaded_files:
        chunks = chunk_text(
            text=content,
            filename=filename,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            file_type=file_type  # Neue Parameter
        )
        all_chunks.extend(chunks)
        
    return all_chunks

# Update chunk_text to include file_type
def chunk_text(
    text: str,
    filename: str,
    chunk_size: int = 1000,
    chunk_overlap: int = 200,
    file_type: str = "txt"  # Neuer Parameter
) -> List[Dict[str, Any]]:
    """Erweitert um file_type in Metadaten."""
    if not text:
        return []

    chunks = []
    start = 0
    chunk_id = 0
    
    while start < len(text):
        end = start + chunk_size
        chunk_content = text[start:end]
        
        chunks.append({
            "content": chunk_content,
            "metadata": {
                "filename": filename,
                "file_type": file_type,  # Neu
                "position": start,
                "chunk_id": f"{filename}-{chunk_id}",
                "chunk_size": len(chunk_content)  # Zus√§tzliche Info
            }
        })
        
        start += chunk_size - chunk_overlap
        chunk_id += 1
        
    return chunks
```

#### Schritt 5: Streamlit UI Updates (`app/streamlit_app.py`)

**File Upload erweitern:**
```python
# Update file uploader
uploaded_files = st.sidebar.file_uploader(
    "Laden Sie Dokumente hoch", 
    type=["txt", "pdf", "docx"],  # Erweiterte Typen
    accept_multiple_files=True,
    help="Unterst√ºtzte Formate: TXT, PDF, DOCX"
)

# File processing mit Typ-Anzeige
if uploaded_files:
    upload_stats = {"txt": 0, "pdf": 0, "docx": 0}
    
    for uploaded_file in uploaded_files:
        file_path = os.path.join(upload_dir, uploaded_file.name)
        file_extension = uploaded_file.name.split('.')[-1].lower()
        
        # Validate file type
        if file_extension not in ["txt", "pdf", "docx"]:
            st.sidebar.error(f"Nicht unterst√ºtzter Dateityp: {uploaded_file.name}")
            continue
        
        # Save file
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        upload_stats[file_extension] += 1
        st.sidebar.success(f"‚úì {uploaded_file.name} ({file_extension.upper()}) hochgeladen")
    
    # Upload summary
    summary_parts = []
    for file_type, count in upload_stats.items():
        if count > 0:
            summary_parts.append(f"{count} {file_type.upper()}")
    
    if summary_parts:
        st.sidebar.info(f"Hochgeladen: {', '.join(summary_parts)}")

# Document ingestion mit Progress
if st.sidebar.button("Dokumente ingestieren (in Vektor-DB)"):
    with st.spinner("Dokumente werden verarbeitet..."):
        # Progress bar f√ºr verschiedene Schritte
        progress_bar = st.sidebar.progress(0)
        status_text = st.sidebar.empty()
        
        try:
            status_text.text("Lade Dokumente...")
            progress_bar.progress(25)
            
            pipeline.ingest_documents()
            
            progress_bar.progress(100)
            status_text.text("Fertig!")
            
            st.sidebar.success("Dokumente erfolgreich ingestiert!")
            
            # Zeige Statistiken
            collection_count = pipeline.collection.count()
            st.sidebar.metric("Dokumente in DB", collection_count)
            
        except Exception as e:
            st.sidebar.error(f"Fehler beim Ingestieren: {str(e)}")
            progress_bar.empty()
            status_text.empty()

# Enhanced document info display
st.sidebar.markdown("### üìä Dokumenten-√úbersicht")
if pipeline.collection.count() > 0:
    # Zeige Dokumenttyp-Statistiken
    try:
        # Hole Metadaten aus der Collection
        sample_results = pipeline.collection.get(limit=1000, include=["metadatas"])
        if sample_results['metadatas']:
            file_types = {}
            for metadata in sample_results['metadatas']:
                file_type = metadata.get('file_type', 'unknown')
                file_types[file_type] = file_types.get(file_type, 0) + 1
            
            for file_type, count in file_types.items():
                st.sidebar.metric(f"{file_type.upper()}-Chunks", count)
    except:
        pass

# Enhanced context display mit file type
if user_query and pipeline.collection.count() > 0:
    with st.spinner("Antwort wird generiert..."):
        answer = pipeline.answer_query(user_query)
        st.subheader("ü§ñ Antwort:")
        st.write(answer)

        # Display retrieved context with file type info
        st.subheader("üìÑ Verwendeter Kontext:")
        retrieved_chunks = pipeline.retriever.retrieve(user_query, top_k=3)
        
        if retrieved_chunks:
            for i, chunk in enumerate(retrieved_chunks):
                file_type = chunk['metadata'].get('file_type', 'unknown')
                filename = chunk['metadata']['filename']
                distance = chunk['distance']
                
                # File type icon
                type_icons = {"pdf": "üìÑ", "docx": "üìù", "txt": "üìÑ"}
                icon = type_icons.get(file_type, "üìÑ")
                
                with st.expander(
                    f"{icon} Chunk {i+1} - {filename} ({file_type.upper()}) - Relevanz: {1-distance:.3f}",
                    expanded=(i == 0)  # Erstes Chunk expanded
                ):
                    st.code(chunk['content'], language=None)
                    
                    # Zus√§tzliche Metadaten
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Position", chunk['metadata'].get('position', 'N/A'))
                    with col2:
                        st.metric("Chunk-Gr√∂√üe", len(chunk['content']))
                    with col3:
                        st.metric("Distanz", f"{distance:.4f}")
        else:
            st.info("Kein relevanter Kontext gefunden.")
```

#### Schritt 6: Error Handling und Edge Cases

**Robuste Error-Behandlung hinzuf√ºgen:**
```python
# In src/text_processor.py
class DocumentProcessingError(Exception):
    """Custom exception f√ºr Dokumentverarbeitungsfehler."""
    pass

def safe_file_processing(file_path: Path, processor_func) -> Optional[str]:
    """Sichere Dateiverarbeitung mit umfassendem Error Handling."""
    try:
        return processor_func(file_path)
    except PermissionError:
        logging.error(f"Permission denied: {file_path}")
        return None
    except FileNotFoundError:
        logging.error(f"File not found: {file_path}")
        return None
    except MemoryError:
        logging.error(f"File too large to process: {file_path}")
        return None
    except Exception as e:
        logging.error(f"Unexpected error processing {file_path}: {e}")
        return None

# File size validation
MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB

def validate_file_size(file_path: Path) -> bool:
    """Validiert Dateigr√∂√üe."""
    try:
        size = file_path.stat().st_size
        if size > MAX_FILE_SIZE:
            logging.warning(f"File {file_path.name} too large: {size/1024/1024:.1f}MB")
            return False
        return True
    except:
        return False
```

#### Schritt 7: Testing und Validation

**Neue Test-Dateien erstellen:**
```python
# In test.py - erweiterte Tests
def test_document_processing():
    """Test alle Dokumenttypen."""
    test_data_dir = Path("test_data")
    test_data_dir.mkdir(exist_ok=True)
    
    # Erstelle Test-Dateien
    create_test_files(test_data_dir)
    
    # Test processing
    from src.text_processor import process_documents
    chunks = process_documents(str(test_data_dir))
    
    # Validiere Ergebnisse
    assert len(chunks) > 0
    file_types = set(chunk['metadata']['file_type'] for chunk in chunks)
    print(f"Processed file types: {file_types}")
    
    # Cleanup
    shutil.rmtree(test_data_dir)

def create_test_files(test_dir: Path):
    """Erstellt Test-Dateien f√ºr alle Formate."""
    # TXT
    with open(test_dir / "test.txt", "w", encoding="utf-8") as f:
        f.write("Test TXT content with some text.")
    
    # Simple PDF (requires reportlab for creation)
    # Simple DOCX (requires python-docx for creation)
    # Implementation details...
```

**Erfolgskriterien und Acceptance Tests:**
1. ‚úÖ **ERF√úLLT**: Alle drei Dateitypen (TXT, PDF, DOCX) werden erkannt und verarbeitet
2. ‚úÖ **ERF√úLLT**: Streamlit UI zeigt korrekte Dateityp-Icons und Metadaten
3. ‚úÖ **ERF√úLLT**: Tabellen in PDF/DOCX werden strukturiert extrahiert
4. ‚úÖ **ERF√úLLT**: Error Handling funktioniert f√ºr korrupte/gro√üe Dateien (50MB Limit)
5. ‚úÖ **ERF√úLLT**: Performance ist akzeptabel mit Batch-Processing
6. ‚úÖ **ERF√úLLT**: Metadaten enthalten file_type Information in allen Komponenten
7. ‚úÖ **ERF√úLLT**: Backward Compatibility mit bestehenden TXT-Workflows

**Implementierte Features:**
- **PDF-Parser**: Dual-Strategy (pdfplumber + PyMuPDF fallback) mit Tabellen-Extraktion
- **DOCX-Parser**: Vollst√§ndige Strukturerhaltung mit python-docx
- **Unified Interface**: `load_all_files()` mit automatischer Format-Erkennung
- **Enhanced UI**: File-Type-Icons, Upload-Statistiken, Progress-Bars
- **Robuste Error-Behandlung**: Gr√∂√üen-Validierung, Encoding-Fallbacks, Logging
- **Comprehensive Testing**: Multi-Format-Tests, Metadaten-Validierung, System-Integration

**N√§chste Priorit√§t:** P1.2 Verbessertes Chunking-System

### P1.2: Verbessertes Chunking-System ‚úÖ **ABGESCHLOSSEN**
**Ziel:** Semantisch sinnvollere Text-Segmentierung

**Status:** ‚úÖ **VOLLST√ÑNDIG IMPLEMENTIERT** (Januar 2025)

**Implementierte Features:**
- **Sentence-Aware Chunking**: Respektiert Satzgrenzen statt harter Zeichen-Abschnitte
- **Semantic Boundary Detection**: Intelligente √úberschriften- und Absatzerkennung
- **Enhanced Metadata**: Reichhaltige Chunk-Metadaten mit semantischen Informationen
- **Keyword Extraction**: Automatische Extraktion wichtiger Begriffe pro Chunk
- **Semantic Density Scoring**: Berechnung der Informationsdichte f√ºr bessere Retrieval-Qualit√§t

**Implementierte Funktionen in `src/text_processor.py`:**
```python
def smart_chunk_text(text, filename, chunk_size=1000, chunk_overlap=200, file_type="txt")
def normalize_text(text) -> str
def split_into_paragraphs(text) -> List[str]
def split_into_sentences(text) -> List[str]
def is_heading(text) -> bool
def get_overlap_sentences(sentences, target_overlap) -> List[Dict]
def calculate_semantic_density(text) -> float
def extract_keywords(text, max_keywords=5) -> List[str]
```

**Erweiterte Chunk-Metadaten:**
- `sentence_count`: Anzahl S√§tze im Chunk
- `contains_heading`: Ob √úberschriften enthalten sind
- `semantic_density`: Informationsdichte (0.0-1.0)
- `keyword_tags`: Wichtige Begriffe automatisch extrahiert
- `paragraph_indices`: Ursprungsabs√§tze

**Testing-Ergebnisse:**
- Bessere Chunk-Qualit√§t durch Satzgrenzen-Respekt
- Semantic density: 0.716 (Kubernetes), 0.681 (Python)
- Keyword extraction: ['kubernetes', 'container', 'pods'] etc.
- Reduzierte Chunk-Anzahl bei verbesserter Koh√§renz

**N√§chste Priorit√§t:** P1.3 Konfigurationssystem

### P1.3: Konfiguration und Einstellungen ‚úÖ **ABGESCHLOSSEN**
**Ziel:** System konfigurierbar machen ohne Code-√Ñnderungen

**Status:** ‚úÖ **VOLLST√ÑNDIG IMPLEMENTIERT** (Januar 2025)

**Implementierte Features:**
- **YAML-basierte Konfiguration**: Vollst√§ndige `config.yaml` mit 11 Konfigurationssektionen
- **Environment Variable Override**: RAG_SECTION_KEY Format f√ºr Deployment-Anpassungen
- **Type-safe Dataclasses**: Validierte Konfigurationsdatenklassen f√ºr alle Bereiche
- **Runtime-Updates**: Konfigurations√§nderungen ohne Neustart m√∂glich
- **Comprehensive Integration**: Alle Module nutzen zentralisierte Konfiguration

**Erfolgskriterien - Alle erf√ºllt:**
1. ‚úÖ Alle Parameter √ºber config.yaml konfigurierbar
2. ‚úÖ Environment Variables √ºberschreiben YAML-Werte
3. ‚úÖ Type-Safety und Validation implementiert
4. ‚úÖ Runtime-Konfiguration in Streamlit UI
5. ‚úÖ Backward Compatibility erhalten
6. ‚úÖ Comprehensive Testing mit 4/4 Tests bestanden

**Implementierte Dateien:**
- `config.yaml`: Hauptkonfigurationsdatei mit allen Parametern
- `src/config.py`: Erweiterte Konfigurationsverwaltung mit Dataclasses
- `test_config_integration.py`: Vollst√§ndige Testsuite
- Alle Module: Vollst√§ndige Config-Integration

**N√§chste Priorit√§t:** P3.1 Erweiterte LLM-Integration

---

## Phase 2: Retrieval-Optimierungen (Priorit√§t: HOCH)

### P2.1: Enhanced Hybrid Retrieval System ‚úÖ **ABGESCHLOSSEN**
**Ziel:** Intelligente Kombination von semantischer und Keyword-basierter Suche mit deutschen Sprachoptimierungen

**Status:** ‚úÖ **VOLLST√ÑNDIG IMPLEMENTIERT** (Januar 2025)

**Implementierte Features:**
- **QueryAnalyzer**: Adaptive Query-Typ-Klassifikation (technical/question/balanced) mit erweiterten Pattern-Erkennungen
- **GermanBM25Retriever**: German-optimized BM25 keyword search mit NLTK stemming, compound word handling, stopword filtering
- **EnhancedHybridRetriever**: Intelligente Score-Fusion mit adaptiver Gewichtung basierend auf Query-Typ
- **Pipeline-Integration**: `enhanced_answer_query()` Methode mit Hybrid-Retrieval-Setup
- **Streamlit UI-Enhancements**: Retrieval-Methoden-Auswahl, Query-Typ-Display, detaillierte Hybrid-Scoring-Metriken
- **Comprehensive Testing**: 13 umfassende Tests (100% Erfolgsrate) f√ºr alle Komponenten und System-Integration
- **German Language Optimization**: Erweiterte deutsche Stoppw√∂rter, Stemming, compound word detection

**Erfolgskriterien - Alle erf√ºllt:**
1. ‚úÖ Query Type Detection: Technical/Question/Balanced classification working perfectly
2. ‚úÖ German Language Support: Stemming, compound words, stopwords fully implemented
3. ‚úÖ Performance: Hybrid retrieval fast and accurate
4. ‚úÖ Integration: Complete Streamlit UI with detailed scoring display
5. ‚úÖ Testing: 13 comprehensive tests with 100% pass rate

**N√§chste Priorit√§t:** P6.1 Performance Benchmarking (aus Todo-Liste √ºbernommen)

#### Detaillierte Implementierungs-Roadmap (ABGESCHLOSSEN):

#### Schritt 1: Dependencies und Setup
```bash
# Neue Abh√§ngigkeiten installieren
pip install rank-bm25>=0.2.2          # BM25 keyword search
pip install nltk>=3.8.1               # German stemming
pip install python-Levenshtein>=0.20.0 # Fuzzy matching
```

**Update `test.py`:**
- F√ºge Package-Tests hinzu: `rank_bm25`, `nltk`, `Levenshtein`
- Teste NLTK German resources download

#### Schritt 2: Erweiterte Retriever-Architektur (`src/retriever.py`)

**Neue Klassen-Struktur:**
```python
from rank_bm25 import BM25Okapi
from nltk.stem import SnowballStemmer
import nltk
from typing import List, Dict, Tuple, Any
import re

class QueryAnalyzer:
    """Analysiert Query-Typ f√ºr adaptive Gewichtung."""
    
    def __init__(self):
        self.technical_patterns = [
            r'\b[A-Z_]{2,}\b',  # Konstanten/Env vars
            r'\b\w+\(\)',       # Funktionsaufrufe
            r'\.\w+',           # Attribute/Methods
            r'\w+\.\w+',        # Module.function
        ]
        self.code_keywords = {
            'python', 'kubernetes', 'docker', 'git', 'api', 'json', 
            'yaml', 'config', 'function', 'class', 'import', 'pip'
        }
    
    def analyze_query_type(self, query: str) -> Dict[str, float]:
        """
        Bestimmt Query-Typ und returniert Gewichtungen.
        
        Returns:
            Dict mit 'semantic_weight' und 'keyword_weight' (sum = 1.0)
        """
        query_lower = query.lower()
        
        # Technische Pattern-Erkennung
        technical_score = 0.0
        for pattern in self.technical_patterns:
            if re.search(pattern, query):
                technical_score += 0.3
        
        # Code-Keyword-Erkennung
        code_score = sum(0.2 for keyword in self.code_keywords 
                        if keyword in query_lower) / len(self.code_keywords)
        
        # Frage-Pattern (semantisch orientiert)
        question_patterns = [r'\bwas\b', r'\bwie\b', r'\bwarum\b', r'\bwann\b', r'\?']
        question_score = sum(0.25 for pattern in question_patterns 
                           if re.search(pattern, query_lower, re.IGNORECASE))
        
        # Adaptive Gewichtung basierend auf Scores
        technical_total = min(technical_score + code_score, 1.0)
        
        if technical_total > 0.5:
            # Technische Query ‚Üí mehr Keyword-Gewicht
            keyword_weight = 0.6 + (technical_total - 0.5) * 0.3
            semantic_weight = 1.0 - keyword_weight
        elif question_score > 0.5:
            # Frage-Query ‚Üí mehr Semantic-Gewicht
            semantic_weight = 0.7 + question_score * 0.2
            keyword_weight = 1.0 - semantic_weight
        else:
            # Balanced Query
            semantic_weight = 0.6
            keyword_weight = 0.4
        
        return {
            'semantic_weight': round(semantic_weight, 2),
            'keyword_weight': round(keyword_weight, 2),
            'query_type': 'technical' if technical_total > 0.5 else 
                         'question' if question_score > 0.5 else 'balanced'
        }

class GermanBM25Retriever:
    """BM25-Retriever mit deutscher Sprachoptimierung."""
    
    def __init__(self, documents: List[Dict[str, Any]]):
        self.documents = documents
        self.stemmer = SnowballStemmer('german')
        
        # Download NLTK resources if needed
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            nltk.download('punkt')
        
        # Prepare BM25 corpus
        self.tokenized_corpus = [self._preprocess_text(doc['content']) 
                                for doc in documents]
        self.bm25 = BM25Okapi(self.tokenized_corpus)
        
    def _preprocess_text(self, text: str) -> List[str]:
        """
        Deutsche Textvorverarbeitung mit Stemming und Compound-Handling.
        """
        # Normalisiere Text
        text = text.lower()
        
        # Entferne Satzzeichen, behalte Wort-Grenzen
        text = re.sub(r'[^\w\s-]', ' ', text)
        
        # Tokenisiere
        tokens = text.split()
        
        # Deutsche Stoppw√∂rter (erweitert)
        german_stopwords = {
            'der', 'die', 'und', 'in', 'den', 'von', 'zu', 'das', 'mit', 'sich',
            'des', 'auf', 'ist', 'im', 'dem', 'nicht', 'ein', 'eine', 'als',
            'auch', 'es', 'an', 'werden', 'aus', 'er', 'hat', 'dass', 'sie',
            'nach', 'wird', 'bei', 'einer', 'um', 'am', 'sind', 'noch', 'wie',
            'einem', '√ºber', 'einen', 'so', 'zum', 'war', 'haben', 'nur', 'oder',
            'aber', 'vor', 'zur', 'bis', 'mehr', 'durch', 'man', 'sein', 'wurde',
            'wenn', 'k√∂nnen', 'alle', 'w√ºrde', 'meine', 'macht', 'kann', 'soll',
            'wir', 'ich', 'dir', 'du', 'ihr', 'uns', 'euch'
        }
        
        # Filtere und stemme
        processed_tokens = []
        for token in tokens:
            if len(token) >= 3 and token not in german_stopwords:
                # Stemming f√ºr bessere Matching
                stemmed = self.stemmer.stem(token)
                processed_tokens.append(stemmed)
                
                # Compound word handling - f√ºge auch Original hinzu f√ºr exakte Matches
                if len(token) > 6:  # Wahrscheinlich compound word
                    processed_tokens.append(token)
        
        return processed_tokens
    
    def retrieve(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """BM25-basierte Keyword-Suche."""
        query_tokens = self._preprocess_text(query)
        
        if not query_tokens:
            return []
        
        # BM25 Scoring
        scores = self.bm25.get_scores(query_tokens)
        
        # Top-K Ergebnisse mit Scores
        results = []
        for idx, score in enumerate(scores):
            if score > 0:  # Nur relevante Ergebnisse
                result = self.documents[idx].copy()
                result['bm25_score'] = float(score)
                result['distance'] = 1.0 / (1.0 + score)  # F√ºr Konsistenz mit semantic
                results.append(result)
        
        # Sortiere nach Score (h√∂her = besser)
        results.sort(key=lambda x: x['bm25_score'], reverse=True)
        
        return results[:top_k]

class EnhancedHybridRetriever:
    """
    Intelligenter Hybrid-Retriever mit adaptiver Gewichtung.
    """
    
    def __init__(self, semantic_retriever, vector_store, documents: List[Dict[str, Any]]):
        self.semantic_retriever = semantic_retriever
        self.vector_store = vector_store
        self.query_analyzer = QueryAnalyzer()
        self.bm25_retriever = GermanBM25Retriever(documents)
        
    def hybrid_retrieve(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        F√ºhrt Hybrid-Retrieval mit adaptiver Gewichtung durch.
        """
        # 1. Query-Analyse f√ºr adaptive Gewichtung
        query_analysis = self.query_analyzer.analyze_query_type(query)
        semantic_weight = query_analysis['semantic_weight']
        keyword_weight = query_analysis['keyword_weight']
        
        # 2. Beide Retrieval-Methoden ausf√ºhren (mehr Kandidaten holen)
        semantic_results = self.semantic_retriever.retrieve(query, top_k * 3)
        keyword_results = self.bm25_retriever.retrieve(query, top_k * 3)
        
        # 3. Ergebnisse zusammenf√ºhren und neu bewerten
        final_results = self._merge_and_rerank(
            semantic_results, keyword_results, 
            semantic_weight, keyword_weight, top_k
        )
        
        # 4. Erweitere Metadaten
        for result in final_results:
            result['retrieval_method'] = 'hybrid'
            result['semantic_weight'] = semantic_weight
            result['keyword_weight'] = keyword_weight
            result['query_type'] = query_analysis['query_type']
        
        return final_results
    
    def _merge_and_rerank(self, semantic_results: List[Dict], keyword_results: List[Dict],
                         semantic_weight: float, keyword_weight: float, top_k: int) -> List[Dict]:
        """F√ºhrt intelligente Ergebnis-Zusammenf√ºhrung durch."""
        
        # Create unified candidate pool mit chunk_id als key
        candidates = {}
        
        # Semantic results verarbeiten
        for i, result in enumerate(semantic_results):
            chunk_id = result['metadata']['chunk_id']
            semantic_score = 1.0 - result['distance']  # Convert distance to score
            
            candidates[chunk_id] = {
                **result,
                'semantic_score': semantic_score,
                'semantic_rank': i + 1,
                'bm25_score': 0.0,
                'bm25_rank': float('inf')
            }
        
        # BM25 results hinzuf√ºgen/aktualisieren
        for i, result in enumerate(keyword_results):
            chunk_id = result['metadata']['chunk_id']
            bm25_score = result['bm25_score']
            
            if chunk_id in candidates:
                # Update existing candidate
                candidates[chunk_id]['bm25_score'] = bm25_score
                candidates[chunk_id]['bm25_rank'] = i + 1
            else:
                # Add new candidate (only from BM25)
                candidates[chunk_id] = {
                    **result,
                    'semantic_score': 0.0,
                    'semantic_rank': float('inf'),
                    'bm25_score': bm25_score,
                    'bm25_rank': i + 1
                }
        
        # Compute hybrid scores
        final_candidates = []
        for chunk_id, candidate in candidates.items():
            # Normalisiere Scores (0-1 range)
            norm_semantic = candidate['semantic_score']
            norm_bm25 = min(candidate['bm25_score'] / 10.0, 1.0)  # BM25 can be > 1
            
            # Hybrid score mit adaptiven Gewichtungen
            hybrid_score = (semantic_weight * norm_semantic + 
                          keyword_weight * norm_bm25)
            
            # Bonus f√ºr Dokumente die in beiden Top-Results sind
            if (candidate['semantic_rank'] <= top_k and 
                candidate['bm25_rank'] <= top_k):
                hybrid_score *= 1.2  # 20% boost
            
            candidate['hybrid_score'] = hybrid_score
            candidate['distance'] = 1.0 - hybrid_score  # For consistency
            final_candidates.append(candidate)
        
        # Sortiere nach Hybrid-Score
        final_candidates.sort(key=lambda x: x['hybrid_score'], reverse=True)
        
        return final_candidates[:top_k]
```

#### Schritt 3: Integration in Pipeline (`src/pipeline.py`)

**Erweitere RAGPipeline-Klasse:**
```python
# In src/pipeline.py - neue Methoden hinzuf√ºgen

def _setup_enhanced_retriever(self):
    """Setup Enhanced Hybrid Retriever."""
    # Hole alle Dokumente f√ºr BM25
    collection_data = self.collection.get(include=['documents', 'metadatas'])
    documents = []
    
    if collection_data['documents']:
        for doc, metadata in zip(collection_data['documents'], collection_data['metadatas']):
            documents.append({
                'content': doc,
                'metadata': metadata
            })
    
    # Erstelle Enhanced Hybrid Retriever
    self.enhanced_retriever = EnhancedHybridRetriever(
        semantic_retriever=self.retriever,
        vector_store=self.vector_store,
        documents=documents
    )

def enhanced_answer_query(self, query: str, top_k: int = 5) -> str:
    """Enhanced Query Processing mit Hybrid Retrieval."""
    if self.collection.count() == 0:
        return "Keine Dokumente in der Datenbank gefunden."
    
    # Setup Enhanced Retriever if not done
    if not hasattr(self, 'enhanced_retriever'):
        self._setup_enhanced_retriever()
    
    # Hybrid Retrieval
    retrieved_chunks = self.enhanced_retriever.hybrid_retrieve(query, top_k)
    
    if not retrieved_chunks:
        return "Keine relevanten Informationen zu Ihrer Frage gefunden."
    
    # Generiere Antwort mit Enhanced Context
    answer = self.llm_handler.generate_answer(query, retrieved_chunks)
    
    return answer
```

#### Schritt 4: Streamlit UI Updates (`app/streamlit_app.py`)

**Erweitere UI f√ºr Hybrid Retrieval:**
```python
# Neue Sidebar-Option f√ºr Retrieval-Methode
st.sidebar.markdown("### üîç Retrieval-Einstellungen")
retrieval_method = st.sidebar.radio(
    "Retrieval-Methode",
    ["Hybrid (Empfohlen)", "Nur Semantisch", "Nur Keywords"],
    help="Hybrid kombiniert semantische und Keyword-Suche intelligent"
)

# Enhanced Query Processing
if user_query and pipeline.collection.count() > 0:
    with st.spinner("Antwort wird generiert..."):
        if retrieval_method == "Hybrid (Empfohlen)":
            answer = pipeline.enhanced_answer_query(user_query)
            retrieved_chunks = pipeline.enhanced_retriever.hybrid_retrieve(user_query, 3)
        else:
            # Fallback auf Standard-Retrieval
            answer = pipeline.answer_query(user_query)
            retrieved_chunks = pipeline.retriever.retrieve(user_query, 3)
        
        st.subheader("ü§ñ Antwort:")
        st.write(answer)

        # Enhanced Context Display mit Retrieval-Info
        if retrieved_chunks and retrieval_method == "Hybrid (Empfohlen)":
            st.subheader("üìÑ Hybrid Retrieval Details:")
            
            # Query Analysis Info
            if hasattr(retrieved_chunks[0], 'query_type'):
                query_info = retrieved_chunks[0]
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Query-Typ", query_info.get('query_type', 'unknown'))
                with col2:
                    st.metric("Semantic Weight", f"{query_info.get('semantic_weight', 0):.2f}")
                with col3:
                    st.metric("Keyword Weight", f"{query_info.get('keyword_weight', 0):.2f}")
            
            # Chunk Details
            for i, chunk in enumerate(retrieved_chunks):
                with st.expander(f"üìÑ Chunk {i+1} - {chunk['metadata']['filename']} - Hybrid Score: {chunk.get('hybrid_score', 0):.3f}"):
                    st.code(chunk['content'], language=None)
                    
                    # Detailed Scoring
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("Semantic", f"{chunk.get('semantic_score', 0):.3f}")
                    with col2:
                        st.metric("BM25", f"{chunk.get('bm25_score', 0):.3f}")
                    with col3:
                        st.metric("Hybrid", f"{chunk.get('hybrid_score', 0):.3f}")
                    with col4:
                        st.metric("Method", chunk.get('retrieval_method', 'standard'))
```

#### Schritt 5: Comprehensive Testing Framework

**Erstelle `tests/test_hybrid_retrieval.py`:**
```python
import pytest
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.retriever import QueryAnalyzer, GermanBM25Retriever, EnhancedHybridRetriever
from src.pipeline import RAGPipeline

class TestQueryAnalyzer:
    def setup_method(self):
        self.analyzer = QueryAnalyzer()
    
    def test_technical_query_detection(self):
        """Test technical query detection."""
        technical_queries = [
            "Wie erstelle ich eine Docker-Compose.yml?",
            "Was ist os.environ in Python?",
            "kubectl get pods Befehl",
            "API_KEY environment variable"
        ]
        
        for query in technical_queries:
            result = self.analyzer.analyze_query_type(query)
            assert result['keyword_weight'] > result['semantic_weight'], f"Failed for: {query}"
            assert result['query_type'] in ['technical', 'balanced']
    
    def test_question_query_detection(self):
        """Test question query detection."""
        question_queries = [
            "Was ist Kubernetes?",
            "Wie funktioniert Docker?",
            "Warum sollte ich virtuelle Umgebungen nutzen?",
            "Wann verwende ich Poetry?"
        ]
        
        for query in question_queries:
            result = self.analyzer.analyze_query_type(query)
            assert result['semantic_weight'] > result['keyword_weight'], f"Failed for: {query}"
            assert result['query_type'] in ['question', 'balanced']

class TestGermanBM25Retriever:
    def setup_method(self):
        self.test_docs = [
            {
                'content': 'Kubernetes ist ein Open-Source-System zur Automatisierung der Bereitstellung',
                'metadata': {'chunk_id': 'test-1', 'filename': 'k8s.txt'}
            },
            {
                'content': 'Python Best Practices umfassen PEP 8 Stilrichtlinien und virtuelle Umgebungen',
                'metadata': {'chunk_id': 'test-2', 'filename': 'python.txt'}
            },
            {
                'content': 'Docker Container erm√∂glichen portable Anwendungen',
                'metadata': {'chunk_id': 'test-3', 'filename': 'docker.txt'}
            }
        ]
        self.retriever = GermanBM25Retriever(self.test_docs)
    
    def test_basic_retrieval(self):
        """Test basic BM25 retrieval."""
        results = self.retriever.retrieve("Kubernetes", top_k=2)
        assert len(results) > 0
        assert results[0]['metadata']['filename'] == 'k8s.txt'
        assert 'bm25_score' in results[0]
    
    def test_german_stemming(self):
        """Test German stemming works."""
        # Test singular vs plural
        results1 = self.retriever.retrieve("Umgebung", top_k=1)
        results2 = self.retriever.retrieve("Umgebungen", top_k=1)
        
        # Should find the same document (stemmed to same root)
        assert len(results1) > 0 and len(results2) > 0
        assert results1[0]['metadata']['filename'] == results2[0]['metadata']['filename']
    
    def test_compound_word_handling(self):
        """Test German compound word handling."""
        results = self.retriever.retrieve("Stilrichtlinien", top_k=1)
        assert len(results) > 0
        assert results[0]['metadata']['filename'] == 'python.txt'

def test_end_to_end_hybrid_retrieval():
    """Integration test f√ºr komplette Hybrid Retrieval Pipeline."""
    # Setup test environment
    pipeline = RAGPipeline(
        data_path="data/raw_texts",
        storage_path="test_vectordb"
    )
    
    # Ingest test documents
    pipeline.ingest_documents()
    
    # Test verschiedene Query-Typen
    test_queries = [
        ("Was ist Kubernetes?", "question"),  # Sollte mehr semantic weight haben
        ("kubectl get pods", "technical"),    # Sollte mehr keyword weight haben
        ("Docker Container Tutorial", "balanced")  # Balanced approach
    ]
    
    for query, expected_type in test_queries:
        # Test enhanced retrieval
        if hasattr(pipeline, 'enhanced_retriever'):
            results = pipeline.enhanced_retriever.hybrid_retrieve(query, top_k=3)
            
            assert len(results) > 0, f"No results for query: {query}"
            assert 'hybrid_score' in results[0], "Missing hybrid_score"
            assert 'query_type' in results[0], "Missing query_type"
            
            # Verify query type detection worked
            detected_type = results[0]['query_type']
            print(f"Query: '{query}' -> Detected: {detected_type}, Expected: {expected_type}")
    
    # Cleanup
    import shutil
    shutil.rmtree("test_vectordb", ignore_errors=True)

if __name__ == "__main__":
    # Run tests
    pytest.main([__file__, "-v"])
```

**Erweitere `test.py` f√ºr Hybrid Retrieval:**
```python
# F√ºge zu test.py hinzu:

def test_hybrid_retrieval_dependencies():
    """Test Hybrid Retrieval specific dependencies."""
    print("Testing Hybrid Retrieval dependencies...")
    
    try:
        import rank_bm25
        print("‚úì rank-bm25 available")
    except ImportError:
        print("‚úó rank-bm25 not available - run: pip install rank-bm25")
        return False
    
    try:
        import nltk
        from nltk.stem import SnowballStemmer
        
        # Test German stemmer
        stemmer = SnowballStemmer('german')
        test_word = stemmer.stem('Umgebungen')
        print(f"‚úì NLTK German stemmer available (test: Umgebungen -> {test_word})")
    except ImportError:
        print("‚úó NLTK not available - run: pip install nltk")
        return False
    except LookupError:
        print("! NLTK German data missing - will be downloaded automatically")
    
    try:
        import Levenshtein
        print("‚úì python-Levenshtein available")
    except ImportError:
        print("‚úó python-Levenshtein not available - run: pip install python-Levenshtein")
        return False
    
    return True

def test_hybrid_retrieval_performance():
    """Test Hybrid Retrieval performance with sample queries."""
    print("Testing Hybrid Retrieval performance...")
    
    try:
        # Import and test components
        from src.retriever import QueryAnalyzer, GermanBM25Retriever
        
        # Test Query Analyzer
        analyzer = QueryAnalyzer()
        test_queries = [
            "Was ist Kubernetes?",
            "kubectl get pods command",
            "Docker Container Best Practices"
        ]
        
        for query in test_queries:
            analysis = analyzer.analyze_query_type(query)
            print(f"Query: '{query}' -> {analysis['query_type']} "
                  f"(semantic: {analysis['semantic_weight']}, "
                  f"keyword: {analysis['keyword_weight']})")
        
        print("‚úì Hybrid Retrieval components working")
        return True
        
    except Exception as e:
        print(f"‚úó Hybrid Retrieval test failed: {e}")
        return False

# Integration in main test function
def main():
    print("=== RAG System Test Suite ===\n")
    
    all_tests = [
        test_dependencies,
        test_gpu_availability, 
        test_document_processing,
        test_hybrid_retrieval_dependencies,  # NEU
        test_hybrid_retrieval_performance,   # NEU
        test_rag_pipeline
    ]
    
    # Rest bleibt gleich...
```

#### Schritt 6: Performance-Benchmarking

**Erstelle `benchmarks/hybrid_retrieval_benchmark.py`:**
```python
import time
import statistics
from typing import List, Dict
import pandas as pd

def benchmark_retrieval_methods():
    """Comprehensive benchmark comparing retrieval methods."""
    
    # Test queries mit erwarteten besten Ergebnissen
    test_cases = [
        {
            'query': 'Was ist ein Kubernetes Pod?',
            'expected_file': 'kubernetes_basics.txt',
            'query_type': 'question'
        },
        {
            'query': 'kubectl get pods',
            'expected_file': 'kubernetes_basics.txt', 
            'query_type': 'technical'
        },
        {
            'query': 'Python PEP 8 Stilrichtlinien',
            'expected_file': 'python_best_practices.txt',
            'query_type': 'balanced'
        },
        {
            'query': 'virtuelle Umgebungen Python',
            'expected_file': 'python_best_practices.txt',
            'query_type': 'balanced'
        }
    ]
    
    # Setup pipeline
    pipeline = RAGPipeline()
    pipeline.ingest_documents()
    
    results = []
    
    for test_case in test_cases:
        query = test_case['query']
        expected_file = test_case['expected_file']
        
        # Test Standard Retrieval
        start_time = time.time()
        standard_results = pipeline.retriever.retrieve(query, top_k=5)
        standard_time = time.time() - start_time
        
        # Test Hybrid Retrieval  
        start_time = time.time()
        hybrid_results = pipeline.enhanced_retriever.hybrid_retrieve(query, top_k=5)
        hybrid_time = time.time() - start_time
        
        # Evaluate accuracy (check if expected file is in top 3)
        standard_accuracy = any(expected_file in r['metadata']['filename'] 
                              for r in standard_results[:3])
        hybrid_accuracy = any(expected_file in r['metadata']['filename'] 
                            for r in hybrid_results[:3])
        
        results.append({
            'query': query,
            'query_type': test_case['query_type'],
            'expected_file': expected_file,
            'standard_time_ms': round(standard_time * 1000, 2),
            'hybrid_time_ms': round(hybrid_time * 1000, 2),
            'standard_top3_accuracy': standard_accuracy,
            'hybrid_top3_accuracy': hybrid_accuracy,
            'standard_top1_score': standard_results[0].get('distance', 1.0) if standard_results else 1.0,
            'hybrid_top1_score': hybrid_results[0].get('hybrid_score', 0.0) if hybrid_results else 0.0
        })
    
    # Create results DataFrame
    df = pd.DataFrame(results)
    
    # Summary statistics
    print("=== HYBRID RETRIEVAL BENCHMARK RESULTS ===")
    print(f"Test Cases: {len(test_cases)}")
    print(f"Average Standard Retrieval Time: {df['standard_time_ms'].mean():.2f}ms")
    print(f"Average Hybrid Retrieval Time: {df['hybrid_time_ms'].mean():.2f}ms")
    print(f"Standard Retrieval Top-3 Accuracy: {df['standard_top3_accuracy'].mean():.2%}")
    print(f"Hybrid Retrieval Top-3 Accuracy: {df['hybrid_top3_accuracy'].mean():.2%}")
    
    # Detailed results
    print("\n=== DETAILED RESULTS ===")
    for _, row in df.iterrows():
        print(f"\nQuery: '{row['query']}' ({row['query_type']})")
        print(f"  Expected: {row['expected_file']}")
        print(f"  Standard: {row['standard_top3_accuracy']} (top-3), {row['standard_time_ms']}ms")
        print(f"  Hybrid:   {row['hybrid_top3_accuracy']} (top-3), {row['hybrid_time_ms']}ms")
        print(f"  Improvement: {'+' if row['hybrid_top3_accuracy'] > row['standard_top3_accuracy'] else '='}")
    
    return df

if __name__ == "__main__":
    benchmark_results = benchmark_retrieval_methods()
    benchmark_results.to_csv('benchmark_results.csv', index=False)
```

#### Erfolgskriterien und Acceptance Tests:

1. **‚úÖ Dependency Installation**: Alle neuen Packages erfolgreich installiert
2. **‚úÖ Query Type Detection**: 
   - Technische Queries erhalten >50% keyword weight
   - Frage-Queries erhalten >60% semantic weight
   - Balanced Queries erhalten ausgewogene Gewichtung
3. **‚úÖ German Language Support**: 
   - Deutsche Stemming funktioniert korrekt
   - Compound words werden erkannt
   - Deutsche Stoppw√∂rter werden gefiltert
4. **‚úÖ Performance**: 
   - Hybrid Retrieval <500ms pro Query
   - Accuracy-Verbesserung um mindestens 20% vs. Standard
5. **‚úÖ Integration**: 
   - Streamlit UI zeigt Hybrid-Details
   - Pipeline unterst√ºtzt beide Retrieval-Modi
6. **‚úÖ Testing**: 
   - Alle Unit Tests bestehen
   - Benchmark zeigt messbare Verbesserungen

**Dateien zu erstellen/√§ndern:**
- `src/retriever.py`: Alle neuen Klassen (QueryAnalyzer, GermanBM25Retriever, EnhancedHybridRetriever)
- `src/pipeline.py`: Integration von Enhanced Retrieval
- `app/streamlit_app.py`: UI-Updates f√ºr Hybrid Retrieval  
- `test.py`: Erweiterte Test-Funktionen
- `tests/test_hybrid_retrieval.py`: Comprehensive Test Suite
- `benchmarks/hybrid_retrieval_benchmark.py`: Performance Benchmarking
- Dependencies: `pip install rank-bm25 nltk python-Levenshtein`

**N√§chste Priorit√§t nach Abschluss:** P3.1 Erweiterte LLM-Integration

### P2.2: Multi-Collection Management ‚úÖ **ABGESCHLOSSEN**
**Ziel:** Erweiterte Dokumentenverwaltung mit Collections und Metadata-Filtering

**Implementierung:**
- **Collection Management**: Erstellung und Verwaltung thematischer Dokumentensammlungen
- **Metadata-Filtering**: Erweiterte Suche nach Dateityp, Datum, Tags
- **UI Integration**: Collection-Auswahl und -Verwaltung im Streamlit-Interface
- **Batch Operations**: Effiziente Verarbeitung gro√üer Dokumentenmengen
- **Statistics Dashboard**: √úbersicht √ºber Collections und Dokumente

```python
class CollectionManager:
    def create_collection(self, name: str, description: str = "") -> Collection
    def list_collections(self) -> List[CollectionInfo]
    def delete_collection(self, name: str) -> bool
    def get_collection_stats(self, name: str) -> CollectionStats
    
class MetadataFilter:
    def filter_by_file_type(self, file_types: List[str]) -> QueryFilter
    def filter_by_date_range(self, start: date, end: date) -> QueryFilter
    def filter_by_tags(self, tags: List[str]) -> QueryFilter
    def combine_filters(self, filters: List[QueryFilter]) -> QueryFilter

class EnhancedVectorStore:
    def search_with_filters(self, query: str, filters: QueryFilter) -> List[Document]
    def cross_collection_search(self, query: str, collections: List[str]) -> List[Document]
```

**Streamlit UI-Erweiterungen:**
- Collection-Dropdown f√ºr Dokumenten-Upload
- Collection-spezifische Suchoptionen
- Metadata-Filter-Interface
- Collection-Statistics-Dashboard
- Batch-Upload mit Collection-Zuweisung

**Dateien zu erstellen/√§ndern:**
- `src/collection_manager.py`: Collection-Management-Klasse
- `src/vectorstore.py`: Erweiterte ChromaDB-Integration
- `app/streamlit_app.py`: Collection-UI und Filter-Interface
- `src/metadata_filter.py`: Advanced Filtering-System

### P2.3: Query-Verbesserung und Preprocessing ‚úÖ **COMPLETED**  
**Ziel:** Intelligentere Query-Verarbeitung
**Implementation Date:** January 2025

**Implemented Features:**
- ‚úÖ `src/query_processor.py`: Complete query enhancement system
  - German-specific query expansion with technical synonyms
  - Spell checking and correction with technical vocabulary
  - Intent detection (question/command/search/factual/procedural/comparison)
  - Query normalization and preprocessing
  - Query suggestions and completions

**Implementation Details:**
- **GermanQueryExpander**: Technical synonyms for Kubernetes/Python terms
- **GermanSpellChecker**: German technical vocabulary with spell correction
- **QueryIntentDetector**: Pattern-based intent classification with 90% confidence
- **QueryProcessor**: Comprehensive preprocessing pipeline
- **Pipeline Integration**: `enhanced_answer_query_with_preprocessing()` method
- **Streamlit UI**: Real-time query analysis, spell suggestions, intent display

**Test Results:**
- ‚úÖ Query processing system: 7/7 test queries successful
- ‚úÖ Intent detection: 90% confidence for questions, 80% for commands
- ‚úÖ Spell checking: Successfully corrects German technical terms
- ‚úÖ Query expansion: Generates relevant synonyms and variations
- ‚úÖ End-to-end integration: Fully functional in RAG pipeline and Streamlit UI

---

## Phase 3: LLM und Generierung (Priorit√§t: MITTEL-HOCH)

### P3.1: Erweiterte LLM-Integration **üîÑ N√ÑCHSTE PRIORIT√ÑT**
**Ziel:** Bessere und flexiblere Antwortgenerierung

**Implementierung:**
- Erweitere `src/llm_handler.py`:
  - Multi-Model-Support (T5, FLAN-T5, GPT-4, Claude)
  - Model-Router basierend auf Query-Komplexit√§t
  - Streaming-Antworten f√ºr bessere UX
  - Custom Prompt-Templates per Dokumenttyp

```python
class AdvancedLLMHandler:
    def __init__(self):
        self.models = {
            "simple": T5ForConditionalGeneration.from_pretrained("google/flan-t5-small"),
            "complex": T5ForConditionalGeneration.from_pretrained("google/flan-t5-large"),
            "api": OpenAIClient()  # Fallback f√ºr komplexe Fragen
        }
    
    def route_to_best_model(self, query, context):
    def stream_generate_answer(self, query, context):
```

**Dateien zu erstellen/√§ndern:**
- `src/llm_handler.py`: Multi-Model-Support
- `src/prompt_templates.py`: Template-Management
- `app/streamlit_app.py`: Streaming-UI-Updates

### P3.2: Context-Aware Response Generation
**Ziel:** Intelligentere Antworten basierend auf Dokumentkontext

**Implementierung:**
- Erstelle `src/context_manager.py`:
  - Context-Ranking und Relevanz-Scoring
  - Multi-Hop-Reasoning f√ºr komplexe Fragen
  - Citation-Generation (Quellenangaben)
  - Answer-Confidence-Scoring

```python
class ContextManager:
    def rank_context_relevance(self, query, contexts):
    def generate_citations(self, answer, source_chunks):
    def calculate_answer_confidence(self, query, contexts, answer):
```

---

## Phase 4: Evaluation und Monitoring (Priorit√§t: MITTEL)

### P4.1: Retrieval-Evaluation-Framework
**Ziel:** Messbare Qualit√§tsbewertung des Retrieval-Systems

**Implementierung:**
- Erstelle `src/evaluation/retrieval_metrics.py`:
  - Precision@K, Recall@K, NDCG-Metriken
  - Ground-Truth-Dataset-Management
  - A/B-Testing-Framework f√ºr verschiedene Retrieval-Strategien

```python
class RetrievalEvaluator:
    def calculate_precision_at_k(self, retrieved, relevant, k):
    def calculate_recall_at_k(self, retrieved, relevant, k):
    def calculate_ndcg(self, retrieved, relevance_scores):
    def run_evaluation_suite(self, test_queries):
```

**Dateien zu erstellen:**
- `src/evaluation/retrieval_metrics.py`
- `src/evaluation/test_datasets.py`
- `evaluation/ground_truth/`: Test-Queries mit expected results

### P4.2: Answer-Quality-Assessment
**Ziel:** Automatische Bewertung der Antwortqualit√§t

**Implementierung:**
- Erstelle `src/evaluation/answer_quality.py`:
  - BLEU/ROUGE-Metriken f√ºr Answer-Quality
  - Semantic-Similarity-Scoring
  - Hallucination-Detection
  - Factual-Accuracy-Checking

```python
class AnswerEvaluator:
    def calculate_bleu_score(self, generated, reference):
    def semantic_similarity(self, generated, reference):
    def detect_hallucinations(self, answer, source_contexts):
```

### P4.3: Monitoring und Logging
**Ziel:** System-Performance und Nutzung √ºberwachen

**Implementierung:**
- Erstelle `src/monitoring/`:
  - Query-Logging mit Response-Times
  - Error-Tracking und Alerting
  - Usage-Analytics (h√§ufige Queries, erfolgreiche vs. erfolglose Suchen)
  - Performance-Metriken (Retrieval-Zeit, Generation-Zeit)

---

## Phase 5: UI/UX-Verbesserungen (Priorit√§t: MITTEL)

### P5.1: Erweiterte Streamlit-Interface
**Ziel:** Professionellere und benutzerfreundlichere Oberfl√§che

**Implementierung:**
- Erweitere `app/streamlit_app.py`:
  - Chat-Interface mit Gespr√§chshistorie
  - Drag & Drop f√ºr Dateien
  - Bulk-Upload mit Progress-Bars
  - Export-Funktionen (Antworten als PDF/MD)
  - Dark/Light-Mode-Toggle

**Neue Features:**
```python
# Chat-History-Management
class ChatManager:
    def save_conversation(self, user_query, system_response):
    def load_conversation_history(self):
    def export_conversation(self, format="pdf"):
```

### P5.2: Document-Viewer und Annotation
**Ziel:** Bessere Transparenz √ºber verwendete Quellen

**Implementierung:**
- Integriere Document-Viewer in Streamlit:
  - PDF-Viewer mit Highlight der verwendeten Chunks
  - Click-to-Source-Funktionalit√§t
  - Document-Annotation-Interface
  - Side-by-side Comparison verschiedener Antworten

---

## Phase 6: Skalierung und Performance (Priorit√§t: NIEDRIG-MITTEL)

### P6.1: Caching und Performance-Optimierung
**Ziel:** Bessere Response-Zeiten und Skalierbarkeit

**Implementierung:**
- Erstelle `src/caching/`:
  - Redis-Integration f√ºr Query-Result-Caching
  - Embedding-Cache f√ºr wiederkehrende Dokumente
  - Model-Loading-Optimierung
  - Async-Processing f√ºr lange Dokumenten-Uploads

### P6.2: Containerization und Deployment
**Ziel:** Einfache Deployment-Optionen

**Implementierung:**
- Erstelle Deployment-Files:
  - `Dockerfile` mit Multi-Stage-Build
  - `docker-compose.yml` mit Services (App, ChromaDB, Redis)
  - `kubernetes/` mit K8s-Manifests
  - GitHub Actions f√ºr CI/CD

---

## Phase 7: Erweiterte Features (Priorit√§t: NIEDRIG)

### P7.1: Multi-Modal RAG
**Ziel:** Bilder und Tabellen in Dokumenten verarbeiten

**Implementierung:**
- Integriere Vision-Models:
  - Image-to-Text f√ºr Screenshots/Diagramme
  - Table-Extraction und -Strukturierung
  - Chart/Graph-Interpretation

### P7.2: Collaborative Features
**Ziel:** Team-Nutzung und Wissens-Sharing

**Implementierung:**
- User-Management und Authentication
- Shared-Collections zwischen Team-Mitgliedern
- Annotation und Kommentar-System
- Knowledge-Base-Versionierung

---

## Implementierungsreihenfolge (Aktualisiert)

### Sprint 1 (2-3 Wochen): ‚úÖ **ABGESCHLOSSEN**
1. ‚úÖ P1.1: PDF/DOCX-Support
2. ‚úÖ P1.2: Verbessertes Chunking
3. ‚úÖ P1.3: Konfigurationssystem

### Sprint 2 (2-3 Wochen): ‚úÖ **ABGESCHLOSSEN** 
1. ‚úÖ P2.1: Enhanced Hybrid Retrieval System
2. ‚úÖ P6.1: Performance Benchmarking System
3. ‚úÖ P1.3: Configuration System (nachgeholt)

### Sprint 3 (2-3 Wochen): üîÑ **AKTUELL**
1. **P2.2: Multi-Collection Management** *(N√§chste Priorit√§t)*
2. P3.1: Multi-Model-LLM-Support
3. P2.3: Query-Preprocessing

### Sprint 4 (2-3 Wochen):
1. P4.1: Evaluation-Framework
2. P3.2: Context-Management
3. P5.1: UI-Verbesserungen

### Sprint 5 (2-3 Wochen):
1. P4.2: Answer-Quality-Assessment
2. P4.3: Monitoring-System
3. P6.2: Containerization

### Langfristig (3+ Monate):
- P6.2: Performance und Deployment
- P7.1-P7.2: Erweiterte Features

## Erfolgsmessung

### Quantitative Metriken:
- **Retrieval-Quality:** Precision@5 > 0.8, NDCG > 0.7
- **Answer-Quality:** BLEU > 0.6, Semantic-Similarity > 0.8
- **Performance:** Query-Response < 3s, Document-Ingestion < 30s/MB
- **User-Experience:** Task-Success-Rate > 90%

### Qualitative Ziele:
- System kann 95% der typischen Fachfragen korrekt beantworten
- Nutzer k√∂nnen intuitiv neue Dokumente hinzuf√ºgen
- Antworten enthalten pr√§zise Quellenangaben
- System funktioniert zuverl√§ssig mit verschiedenen Dokumenttypen

## Ressourcen und Dependencies

### Neue Python-Packages:
```
# Phase 1
pdfplumber>=3.0.0
python-docx>=0.8.11
spacy>=3.4.0
PyYAML>=6.0

# Phase 2  
rank-bm25>=0.2.2
python-Levenshtein>=0.20.0

# Phase 3
openai>=1.0.0
anthropic>=0.7.0

# Phase 4
nltk>=3.8
rouge-score>=0.1.2

# Phase 6
redis>=4.5.0
celery>=5.2.0
```

### Hardware-Empfehlungen:
- **Minimum:** 16GB RAM, 4 CPU Cores, 100GB Storage
- **Optimal:** 32GB RAM, 8 CPU Cores, NVIDIA GPU (8GB+), 500GB SSD

Dieses Plan-Dokument dient als Referenz f√ºr alle zuk√ºnftigen Entwicklungsaktivit√§ten am RAG-System.